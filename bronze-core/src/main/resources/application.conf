spark {
  spark.app.name = "Bronze"
  spark.executor.instances = 2
  spark.executor.cores = 1
  spark.executor.memory = "1g"

}

input {
  hdfs {
    result_table_name = "iris_origin"
    path = "hdfs://node02:9000/ml/lr_data/iris.csv"
    format = "csv"
  }
}

transform {
  schema {
    fields = "calyxLength:Double,calyxWidth:Double,petalLength:Double,petalWidth:Double,classification:String"
    result_table_name = "iris"
  }

  #sql {
  #  sql = "select * from iris where classification <> 'Iris-setosa'"
  #}

  stringIndexer {
    inputCol = "classification"
    outputCol = "category"
    handleInvalid = "error"
  }

  labeledPoint {
    label = "category"
    features = "calyxLength,calyxWidth,petalLength,petalWidth"
  }

  split {
    weights = "0.7,0.3"
    split = true
  }
}

train {
  logisticRegression {
    labelCol = "label"
    featuresCol = "features"
    family = "multinomial"
  }
}

model {
  saveModel {
    path = "hdfs://node02:9000/ml/model/iris"
  }
}

validate {
  binaryClassificationValidate {
    modelType = "logisticRegression"
  }
}

output {
  stdout {
    limit= 100
  }

  # you can also you other output plugins, such as sql
  # hdfs {
  #   path = "hdfs://hadoop-cluster-01/nginx/accesslog_processed"
  #   save_mode = "append"
  # }
}
