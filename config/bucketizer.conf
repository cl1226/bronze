spark {
  spark.app.name = "Bronze"
  spark.executor.instances = 2
  spark.executor.cores = 1
  spark.executor.memory = "1g"

}

input {
  file {
      result_table_name = "test_data"
      path = "/C:\\Users\\SV00242152\\Desktop\\test3.csv"
      format = "csv",
      header = true
    }
}

transform {
  schema {
    fields = "id:Double"
  }
  bucketizer {
    inputCol = "id"
    outputCol = "b_id"
    splits = "-0.5,0.0,0.5"
  }
}

ml {

}

output {
  stdout {
    limit= 10
  }

  # you can also you other output plugins, such as sql
  # hdfs {
  #   path = "hdfs://hadoop-cluster-01/nginx/accesslog_processed"
  #   save_mode = "append"
  # }
}
